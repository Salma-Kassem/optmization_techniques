{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3wjcVeOh6/ISZ5yWCk8M1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Salma-Kassem/optmization_techniques/blob/main/LogisticRergression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import (make_classification , load_breast_cancer,make_moons)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "class LogisticRegressionGD:\n",
        "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = 0\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def loss(self, y, y_pred):\n",
        "        m = len(y)\n",
        "        return -(1/m) * np.sum(y * np.log(y_pred + 1e-15) + (1 - y) * np.log(1 - y_pred + 1e-15))  # +eps for stability\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        m, n = X.shape\n",
        "        self.weights = np.zeros(n)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            z = np.dot(X, self.weights) + self.bias\n",
        "            y_pred = self.sigmoid(z)\n",
        "\n",
        "            current_loss = self.loss(y, y_pred)\n",
        "\n",
        "            dw = (1/m) * np.dot(X.T, (y_pred - y))\n",
        "            db = (1/m) * np.sum(y_pred - y)\n",
        "\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {current_loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        return (self.sigmoid(z) >= 0.5).astype(int)\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_class, y_class = make_classification(n_samples=500, n_features=10, n_classes=2, random_state=0)\n",
        "# Standardize the features (important for gradient descent)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegressionGD(learning_rate=0.01, epochs=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "loss = log_loss(y_test, y_pred)\n",
        "print(f\"Log Loss (binary cross-entropy): {loss:.4f}\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "#Accuracy = (TP + TN) / (TP + TN + FP + FN) Precision = TP / (TP + FP) Recall = TP / (TP + FN) F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred)\n",
        "rec = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy:  {acc:.2f}\")\n",
        "print(f\"Precision: {prec:.2f}\")\n",
        "print(f\"Recall:    {rec:.2f}\")\n",
        "print(f\"F1 Score:  {f1:.2f}\")\n",
        "class LinearRegression:\n",
        "     def __init__(self):\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "     def fit(self, X, y):\n",
        "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "        # Normal Equation: θ = (XᵀX)⁻¹Xᵀy instead of gradient descent (itterative method) for optimal weights and bias [no loop no learning rate ][don't work well with large data ,numerically tricky]\n",
        "        theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
        "\n",
        "        # Save bias and weights\n",
        "        self.bias = theta_best[0]\n",
        "        self.weights = theta_best[1:]\n",
        "     def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "     def score(self, X, y):\n",
        "         y_pred = self.predict(X)\n",
        "         ss_total = np.sum((y-np.mean(y)) ** 2)\n",
        "         ss_res = np.sum((y - y_pred) ** 2)\n",
        "         r2 = 1 - (ss_res / ss_total)\n",
        "         return r2"
      ],
      "metadata": {
        "id": "Xe_gKN17UOOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ca1df7-3f65-40cb-94d8-e27fafc8a5d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6931\n",
            "Epoch 100, Loss: 0.2522\n",
            "Epoch 200, Loss: 0.1897\n",
            "Epoch 300, Loss: 0.1615\n",
            "Epoch 400, Loss: 0.1448\n",
            "Epoch 500, Loss: 0.1336\n",
            "Epoch 600, Loss: 0.1253\n",
            "Epoch 700, Loss: 0.1190\n",
            "Epoch 800, Loss: 0.1139\n",
            "Epoch 900, Loss: 0.1097\n",
            "Log Loss (binary cross-entropy): 0.3162\n",
            "Confusion Matrix:\n",
            "[[42  1]\n",
            " [ 0 71]]\n",
            "Accuracy:  0.99\n",
            "Precision: 0.99\n",
            "Recall:    1.00\n",
            "F1 Score:  0.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqKtWMJnUHrm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77e5cd3-1662-45a4-eec6-50d05339c4d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.1375\n",
            "Epoch 100, Loss: 0.0481\n",
            "Epoch 200, Loss: 0.0437\n",
            "Epoch 300, Loss: 0.0411\n",
            "Epoch 400, Loss: 0.0392\n",
            "Epoch 500, Loss: 0.0377\n",
            "Epoch 600, Loss: 0.0365\n",
            "Epoch 700, Loss: 0.0353\n",
            "Epoch 800, Loss: 0.0344\n",
            "Epoch 900, Loss: 0.0335\n",
            "Confusion Matrix:\n",
            "[[42  1]\n",
            " [ 2 69]]\n",
            "Accuracy:  0.97\n",
            "Precision: 0.99\n",
            "Recall:    0.97\n",
            "F1 Score:  0.98\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import (make_classification , load_breast_cancer,make_moons)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import log_loss, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "class LogisticRegressionSGD:\n",
        "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = 0\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def loss(self, y, y_pred):\n",
        "        m = len(y)\n",
        "        return -(1/m) * np.sum(\n",
        "            y * np.log(y_pred + 1e-15) +\n",
        "            (1 - y) * np.log(1 - y_pred + 1e-15)\n",
        "        )  # Adding a small constant for numerical stability\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        m, n = X.shape\n",
        "        self.weights = np.zeros(n)\n",
        "        self.bias = 0\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Shuffle the training data for stochastic gradient descent\n",
        "            indices = np.random.permutation(m)\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            for i in range(m):  # Loop through each training sample\n",
        "                xi = X_shuffled[i:i+1]  # Extract one sample\n",
        "                yi = y_shuffled[i:i+1]  # Extract the corresponding label\n",
        "\n",
        "                # Compute prediction for the current sample\n",
        "                z = np.dot(xi, self.weights) + self.bias\n",
        "                y_pred = self.sigmoid(z)\n",
        "\n",
        "                # Compute gradients for the current sample\n",
        "                dw = xi.T * (y_pred - yi)  # Gradient for weights\n",
        "                db = y_pred - yi  # Gradient for bias\n",
        "\n",
        "                # Flatten dw to make sure it matches the shape of self.weights\n",
        "                self.weights -= self.learning_rate * dw.flatten()\n",
        "                self.bias -= self.learning_rate * db\n",
        "\n",
        "            # Optionally print the loss every 100 epochs\n",
        "            if epoch % 100 == 0:\n",
        "                y_pred = self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
        "                current_loss = self.loss(y, y_pred)\n",
        "                print(f\"Epoch {epoch}, Loss: {current_loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        return (self.sigmoid(z) >= 0.5).astype(int)\n",
        "\n",
        "# Load and preprocess data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_class, y_class = make_classification(n_samples=500, n_features=10, n_classes=2, random_state=0)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model using Stochastic Gradient Descent\n",
        "model_sgd = LogisticRegressionSGD(learning_rate=0.01, epochs=1000)\n",
        "model_sgd.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model_sgd.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "loss = log_loss(y_test, y_pred)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred)\n",
        "rec = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "#Accuracy = (TP + TN) / (TP + TN + FP + FN) Precision = TP / (TP + FP) Recall = TP / (TP + FN) F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred)\n",
        "rec = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy:  {acc:.2f}\")\n",
        "print(f\"Precision: {prec:.2f}\")\n",
        "print(f\"Recall:    {rec:.2f}\")\n",
        "print(f\"F1 Score:  {f1:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import (make_classification , load_breast_cancer,make_moons)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import log_loss, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "class LogisticRegressionMiniBatchGD:\n",
        "    def __init__(self, learning_rate=0.01, epochs=1000, batch_size=32):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.weights = None\n",
        "        self.bias = 0\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def loss(self, y, y_pred):\n",
        "        m = len(y)\n",
        "        return -(1/m) * np.sum(\n",
        "            y * np.log(y_pred + 1e-15) +\n",
        "            (1 - y) * np.log(1 - y_pred + 1e-15)\n",
        "        )  # Adding a small constant for numerical stability\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        m, n = X.shape\n",
        "        self.weights = np.zeros(n)\n",
        "        self.bias = 0\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Shuffle the training data for mini-batch gradient descent\n",
        "            indices = np.random.permutation(m)\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            # Process data in mini-batches\n",
        "            for i in range(0, m, self.batch_size):\n",
        "                # Create mini-batch\n",
        "                X_batch = X_shuffled[i:i + self.batch_size]\n",
        "                y_batch = y_shuffled[i:i + self.batch_size]\n",
        "\n",
        "                # Compute predictions for the mini-batch\n",
        "                z = np.dot(X_batch, self.weights) + self.bias\n",
        "                y_pred = self.sigmoid(z)\n",
        "\n",
        "                # Compute gradients for the mini-batch\n",
        "                dw = np.dot(X_batch.T, (y_pred - y_batch)) / len(y_batch)  # Gradient for weights\n",
        "                db = np.sum(y_pred - y_batch) / len(y_batch)  # Gradient for bias\n",
        "\n",
        "                # Update weights and bias using the gradients\n",
        "                self.weights -= self.learning_rate * dw\n",
        "                self.bias -= self.learning_rate * db\n",
        "\n",
        "            # Optionally print the loss every 100 epochs\n",
        "            if epoch % 100 == 0:\n",
        "                y_pred = self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
        "                current_loss = self.loss(y, y_pred)\n",
        "                print(f\"Epoch {epoch}, Loss: {current_loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        return (self.sigmoid(z) >= 0.5).astype(int)\n",
        "\n",
        "# Load and preprocess data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_class, y_class = make_classification(n_samples=500, n_features=10, n_classes=2, random_state=0)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model using Mini-Batch Gradient Descent\n",
        "model_mbgd = LogisticRegressionMiniBatchGD(learning_rate=0.01, epochs=1000, batch_size=32)\n",
        "model_mbgd.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model_mbgd.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "loss = log_loss(y_test, y_pred)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred)\n",
        "rec = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "#Accuracy = (TP + TN) / (TP + TN + FP + FN) Precision = TP / (TP + FP) Recall = TP / (TP + FN) F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred)\n",
        "rec = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy:  {acc:.2f}\")\n",
        "print(f\"Precision: {prec:.2f}\")\n",
        "print(f\"Recall:    {rec:.2f}\")\n",
        "print(f\"F1 Score:  {f1:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9qFCc_bAWON",
        "outputId": "14995b5d-916e-4430-ad0c-62f049a74dfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.4955\n",
            "Epoch 100, Loss: 0.0939\n",
            "Epoch 200, Loss: 0.0787\n",
            "Epoch 300, Loss: 0.0718\n",
            "Epoch 400, Loss: 0.0675\n",
            "Epoch 500, Loss: 0.0645\n",
            "Epoch 600, Loss: 0.0623\n",
            "Epoch 700, Loss: 0.0606\n",
            "Epoch 800, Loss: 0.0591\n",
            "Epoch 900, Loss: 0.0579\n",
            "Confusion Matrix:\n",
            "[[42  1]\n",
            " [ 0 71]]\n",
            "Accuracy:  0.99\n",
            "Precision: 0.99\n",
            "Recall:    1.00\n",
            "F1 Score:  0.99\n"
          ]
        }
      ]
    }
  ]
}